# Claude Context - Task Force Purple

**Purpose**: Essential operational knowledge for AI assistant context. Gitignored, never commit.

---

## Critical API Endpoints

### Individual Member Update
```bash
curl -X POST "https://taskforce-purple-api.dev-a4b.workers.dev/api/update-member/@sensanders" \
  -H "Authorization: Bearer taskforce_purple_2025_update"
```
- Updates single member (both Phase 1 + Phase 2)
- Syncs to main storage (`members:all`)
- Auto-recalculates tiers
- Accepts `@username` or `@BIOGUIDE_ID` format

### Batch FEC Update (Cron/Manual)
```bash
curl -X POST "https://taskforce-purple-api.dev-a4b.workers.dev/api/update-fec-batch?batch=3" \
  -H "Authorization: Bearer taskforce_purple_2025_update"
```
- Processes 3 members per run (configurable, max 10)
- Two-phase: financial data → PAC details
- Incremental progress saved to KV
- Rate-limit compliant (15s delays)

### Reset PAC Data
```bash
curl -X POST "https://taskforce-purple-api.dev-a4b.workers.dev/api/reset-pac-data" \
  -H "Authorization: Bearer taskforce_purple_2025_update"
```
- Clears all `pacContributions` arrays
- Rebuilds Phase 2 queue
- Use when PAC filtering logic changes

### Remove Member
```bash
curl -X POST "https://taskforce-purple-api.dev-a4b.workers.dev/api/remove-member/S000033" \
  -H "Authorization: Bearer taskforce_purple_2025_update"
```
- Removes member from `members:all` storage
- Forces fresh data fetch on next update
- Use for corrupted data

### Query Members
```bash
# All members
curl -s "https://taskforce-purple-api.dev-a4b.workers.dev/api/members"

# Single member
curl -s "https://taskforce-purple-api.dev-a4b.workers.dev/api/members/S000033"
```

---

## Known Active Issues

### 1. Chamber Misattribution (Bernie = House, should be Senate)
**Cause**: Congress.gov API members list endpoint returns different structure than individual endpoint
- List endpoint: `member.terms?.item?.[0]?.chamber`
- Individual endpoint: `member.terms?.[0]?.chamber`
- Worker uses list endpoint, gets wrong chamber for multi-term members
**Impact**: Display only (cosmetic)
**Priority**: Low

### 2. FEC Totals vs Schedule A Mismatch
**Fixed**: 2025-10-04
- FEC totals endpoint (`other_political_committee_contributions`) can be $0
- Schedule A itemized data shows actual PAC contributions ($82,500)
- **Solution**: Phase 2 now recalculates `pacMoney` from `pacContributions` array
- Redeploy all members if processed before this fix

---

## Data Pipeline Architecture

### Phase 1: Financial Data (3 FEC calls/member)
1. Candidate lookup: `name + state + chamber` → `candidate_id`
2. Committee financial summary: `candidate_id` → `totalRaised`, FEC `pacMoney`
3. Itemized contributions: Committee ID → filter for actual individual donations

**Output**: `totalRaised`, `grassrootsDonations`, `grassrootsPercent`, `pacMoney` (FEC totals), `tier`

### Phase 2: PAC Enhancement (2-5 FEC calls/member)
1. Schedule A PAC contributions: Committee ID → itemized PAC transactions
2. Committee metadata: Per unique PAC → `committee_type`, `designation`
3. **Recalculate pacMoney**: Sum of actual `pacContributions` (overrides Phase 1 FEC totals)
4. Apply transparency weights to PAC amounts

**Output**: `pacContributions[]`, corrected `pacMoney`, updated `grassrootsPercent`, recalculated `tier`

### PAC Filtering (FEC Line Numbers)
```javascript
// Exclude conduit/earmarked (ACTBLUE, WINRED)
if (contrib.line_number === '11AI') return false;

// Exclude other receipts (bank interest, dividends)
if (contrib.line_number === '15') return false;

// Exclude committee transfers
if (['12', '16', '17', '18'].includes(contrib.line_number)) return false;

// Exclude earmarked pass-throughs
if (contrib.conduit_committee_id) return false;
```

**Never use hardcoded text patterns for filtering - use FEC metadata fields only**

### Tier Calculation: Funding Diffusion Model

Tiers reflect **funding diffusion** - whether power comes from many small donors (democratic) or concentrated sources (corporate/wealthy capture).

**Formula**: Base grassroots % adjusted by transparency penalty

**Transparency Penalty Weights**:
```javascript
// Large Individual Donations (>$200)
0.3x weight - reflects class concentration concern

// PAC Committee Type
'O' (Super PAC): 2.0x penalty - dark money
'P' (Candidate): 0.15x discount (85% off) - personal committees

// PAC Designation
'D' (Leadership PAC): 1.5x penalty - politician-controlled influence
'B' (Lobbyist PAC): 1.5x penalty - corporate lobbying arms
'P' (Principal): 0.15x discount - authorized committees
'A' (Authorized): 0.15x discount - authorized committees
Default: 1.0x - standard institutional influence
```

**Penalty Calculation**:
1. Multiply large donor $ by 0.3
2. For each PAC, multiply $ by transparency weight (if > 1.0)
3. Sum all weighted concerning money
4. Divide by totalRaised to get penalty % (max 30 points)
5. Add penalty to baseline tier thresholds

**Example**: Member with 7% grassroots, 49% large donors, 4% PACs
- Large donor penalty: 49% × 0.3 = ~15 points
- PAC penalty: 4% weighted = ~6 points
- Total penalty: 21 points
- E tier threshold: 15 + 21 = 36% grassroots required
- Actual: 7% < 36% → **F tier**

---

## Rate Limiting

### FEC API
- **Limit**: 1,000 requests/hour (16.67/min)
- **Our usage**: 0.8 calls/min (15-second delays)
- **Safety margin**: 95% under limit

### Smart Batching Strategy
- **Cron schedule**: `*/15 * * * *` (every 15 minutes)
- **Batch size**: 3 Phase 1 + 1 Phase 2 = ~13 FEC calls/run
- **Completion time**: 2-3 days for 538 members

### Cloudflare Worker Limits
- **CPU time**: 30 seconds max
- **Subrequests**: ~50 safe limit
- **Strategy**: Small batches with incremental saves

---

## Storage Structure (KV)

### Main Dataset
**Key**: `members:all`
```json
[
  {
    "bioguideId": "S000033",
    "name": "Sanders, Bernard",
    "totalRaised": 8207886.33,
    "pacMoney": 82500.01,  // Recalculated from pacContributions in Phase 2
    "grassrootsPercent": 98.99,
    "tier": "S",
    "pacContributions": [...],  // Array of 20 PAC objects with metadata
    "pacDetailsStatus": "complete"
  }
]
```

### Processing Queues
**Key**: `processing_queue_phase1` - Members needing financial data
**Key**: `processing_queue_phase2` - Members needing PAC details

### Progress Tracking
**Key**: `batch_progress`
```json
{
  "lastProcessedIndex": 15,
  "phase": "financial"
}
```

---

## Deployment

### Pause Cron
```toml
# wrangler.toml
# [triggers]
# crons = ["*/15 * * * *"]
```
Deploy: `wrangler deploy`

### Enable Cron
```toml
# wrangler.toml
[triggers]
crons = ["*/15 * * * *"]
```
Deploy: `wrangler deploy`

### Frontend Deploy
```bash
npm run build
wrangler pages deploy dist --project-name taskforce-purple
```

---

## Common Operations

### Fix All Members After Code Change
1. Pause cron: Comment out `[triggers]` in wrangler.toml
2. Deploy fix: `wrangler deploy`
3. Reset PAC data: `curl -X POST .../api/reset-pac-data` (if PAC logic changed)
4. Test single member: `curl -X POST .../api/update-member/@sensanders`
5. Enable cron: Uncomment `[triggers]`, `wrangler deploy`
6. Monitor: Check logs with `wrangler tail`

### Debug Individual Member
```bash
# Get current data
curl -s "https://taskforce-purple-api.dev-a4b.workers.dev/api/members/S000033" | jq

# Remove and refresh
curl -X POST "https://taskforce-purple-api.dev-a4b.workers.dev/api/remove-member/S000033" \
  -H "Authorization: Bearer taskforce_purple_2025_update"

curl -X POST "https://taskforce-purple-api.dev-a4b.workers.dev/api/update-member/@sensanders" \
  -H "Authorization: Bearer taskforce_purple_2025_update"
```

### Check Processing Status
```bash
curl -s "https://taskforce-purple-api.dev-a4b.workers.dev/api/status" | jq
```

---

## Important Reminders

1. **NEVER hardcode filters** - Use FEC line numbers and metadata fields
2. **Phase 2 recalculates pacMoney** - Don't trust FEC totals endpoint
3. **Test on Bernie first** - He's the canary (high-profile, edge cases)
4. **Pause cron before fixes** - Prevent bad data propagation
5. **Individual updates sync to main storage** - Use for testing
6. **Check this file before starting work** - Context continuity

---

## Itemized Donor Concentration Analysis (Prototype)

**Status**: Working prototype with automatic cron processing
**Worker**: `taskforce-purple-itemized-prototype`
**URL**: https://taskforce-purple-itemized-prototype.dev-a4b.workers.dev
**Cron**: Every 2 minutes (`*/2 * * * *`)

### Purpose
Fetch ALL Schedule A individual transactions to distinguish concentrated vs. broad donor bases:
- **Problem**: Pelosi (41.2% itemized) and Sanders (41.4% itemized) appear identical
- **Reality**: Sanders likely has 13K middle-class donors, Pelosi likely 2K wealthy donors
- **Solution**: Fetch raw transaction data, deduplicate by contributor name

### Cloudflare Worker Limits (TESTED)

**Hard Limits:**
- **Wall-clock timeout**: 30 seconds maximum
- **Subrequest limit**: 50 per request
- **KV value size**: 25 MB per key ⚠️ **CRITICAL**
- **CPU time**: 10ms (not the blocker - I/O wait is)

**FEC API Reality:**
- **Response time**: 2-19 seconds per page (highly variable)
- **Pagination**: 100 transactions per page
- **Bernie Sanders**: 176 pages (17,566 transactions, ~77 MB raw)
- **Nancy Pelosi**: Unknown (likely 150-200 pages)

**Working Configuration:**
- **Pages per run**: 5 pages (~500 transactions)
- **Processing time**: ~14-27 seconds (safe margin under 30s timeout)
- **Subrequests used**: 5-7 (committee lookup + 5 pages)
- **Cron frequency**: Every 2 minutes
- **Runs needed**: ~36 for Sanders (176 pages ÷ 5 = 35.2 runs)
- **Total time**: ~72 minutes per member (36 runs × 2 min/run)

### Useful Commands

**Check processing status:**
```bash
curl -s "https://taskforce-purple-itemized-prototype.dev-a4b.workers.dev/status"
```

**Manually trigger processing (cron runs automatically):**
```bash
curl -s "https://taskforce-purple-itemized-prototype.dev-a4b.workers.dev/analyze"
```

**Check KV keys for a member:**
```bash
# List all transaction chunks for Bernie
wrangler kv key list --namespace-id=8318226115e2423ab5d141adfa5419f9 --prefix="transactions:S000033:" --remote

# Check progress metadata
wrangler kv key get "itemized_progress:S000033" --namespace-id=8318226115e2423ab5d141adfa5419f9 --remote

# Check analysis results
wrangler kv key get "itemized_analysis:S000033" --namespace-id=8318226115e2423ab5d141adfa5419f9 --remote

# Get specific chunk
wrangler kv key get "transactions:S000033:chunk_000" --namespace-id=8318226115e2423ab5d141adfa5419f9 --remote
```

**Delete keys (for testing/reset):**
```bash
# Delete progress (forces fresh start)
wrangler kv key delete "itemized_progress:S000033" --namespace-id=8318226115e2423ab5d141adfa5419f9 --remote

# Delete specific chunk
wrangler kv key delete "transactions:S000033:chunk_000" --namespace-id=8318226115e2423ab5d141adfa5419f9 --remote
```

**Check worker logs:**
```bash
wrangler tail taskforce-purple-itemized-prototype --format pretty
```

### KV Storage Structure (Chunked Architecture)

**Why Chunked Storage?** KV has a 25 MB per value limit. Bernie's full transaction data is ~77 MB, so we split it across multiple keys.

**Progress tracking (metadata only):**
```
Key: itemized_progress:S000033
Value: {
  bioguideId: "S000033",
  committeeId: "C00411330",
  cycle: 2026,
  currentPage: 111,
  totalPages: 176,
  totalTransactions: 11000,
  totalChunks: 11,
  complete: false,
  startedAt: "2025-10-17T14:42:32Z",
  lastUpdated: "2025-10-18T21:40:12Z"
}
```

**Transaction chunks (1000 transactions each, ~1.5 MB):**
```
Key: transactions:S000033:chunk_000
Value: [...100 transactions...]  // First 1000 transactions

Key: transactions:S000033:chunk_001
Value: [...100 transactions...]  // Next 1000 transactions

...

Key: transactions:S000033:chunk_017
Value: [...566 transactions...]  // Final chunk (17,566 total ÷ 1000)
```

**Analysis results (when complete):**
```
Key: itemized_analysis:S000033
Value: {
  bioguideId: "S000033",
  committeeId: "C00411330",
  cycle: 2026,
  totalTransactions: 17566,
  totalChunks: 18,
  analysis: {
    uniqueDonors: 13247,
    totalAmount: 3400000,
    avgDonation: 256.50,
    medianDonation: 200,
    top10Donors: [...],
    top10Percent: 5.2
  },
  fetchedAt: "2025-10-17T14:42:32Z",
  completedAt: "2025-10-18T22:30:00Z"
}
```

### What We Learn from Raw Data

Once complete, we can calculate:
- **Unique donor count** (deduplicate by contributor_name)
- **Top donor concentration** (top 10 donors = X% of total)
- **Average donation** (proxy for wealth concentration)
- **Median donation** (distribution center)
- **Gini coefficient** (inequality measure)
- **Repeat donor patterns** (1-time vs sustained support)

**This proves with hard data whether itemized donations come from many middle-class donors or few wealthy donors.**

### Architecture Details

**Chunked Processing Strategy:**
1. Worker invoked every 2 minutes by cron
2. Fetches 5 FEC API pages (500 transactions)
3. Stores transactions in buffer
4. When buffer reaches 1,000 transactions, saves chunk to KV
5. Saves lightweight progress metadata (no transaction data)
6. Repeats until all pages fetched
7. On completion: loads all chunks, runs analysis, stores results

**Why This Works:**
- **30s timeout**: 5 pages × 2-5s/page = 14-27s ✓
- **50 subrequest limit**: 5-7 requests/run ✓
- **25 MB KV limit**: 1000 transactions = ~1.5 MB/chunk ✓
- **Automatic**: Cron runs in background, no manual intervention ✓

### Critical Issue Solved: 25 MB KV Limit

**Discovery (2025-10-18):**
- Worker failed silently at page 56/176
- Error: `KV PUT failed: 413 Value length of 26495011 exceeds limit of 26214400`
- **Root cause**: Original design accumulated all transactions in single KV value
- Bernie's full data = ~77 MB (3× over limit)

**Solution:**
- Implemented chunked storage (1000 transactions per key)
- Progress metadata separated from transaction data
- Each chunk ~1.5 MB (well under 25 MB limit)
- Worker successfully resumed and completed processing

**Lesson:** Always account for KV value size limits when storing large datasets.

### Known Limitations

1. **Contributor name deduplication is imperfect**
   - FEC records: "JOHN SMITH", "Smith, John", "J. Smith" are same person
   - Current: Simple uppercase + trim (good enough for concentration metrics)
   - Future: Fuzzy matching if needed

2. **Processing time**
   - ~72 minutes per member (36 runs × 2 min/run)
   - For all 535 members: Would need different strategy (selective analysis)
   - Prototype hardcoded to Bernie + Pelosi only

3. **FEC API response time variability**
   - Some pages take 2s, others take 19s
   - Occasional timeouts possible (worker auto-retries on next cron)

### Current Status (2025-10-18)

**Bernie Sanders:** 111/176 pages (63%), 5,000 transactions collected, ~26 min to completion
**Nancy Pelosi:** Not started (will begin automatically after Bernie completes)

### Next Steps (If Pursuing Full Implementation)

See GitHub Issue #20 for full architecture:
1. ✅ ~~Add cron scheduling~~ (COMPLETE: running every 2 min)
2. ✅ ~~Solve KV storage limits~~ (COMPLETE: chunked storage)
3. Get enhanced FEC API rate limit (email request, free) - optional
4. Create separate KV namespace for itemized data - optional
5. Implement queue-based processing for all 535 members
6. Integrate concentration metrics into tier calculation
7. Add cycle-based data lifecycle (keep current + previous cycle only)

---

## Reference Docs

- **API_STRUCTURES.md**: Comprehensive API documentation (includes API keys in curl examples)
- **SMART_BATCHING_STRATEGY.md**: Rate limiting and batch processing details
- **GRASSROOTS_CALCULATION_GUIDE.md**: PAC type classifications
- **README.md**: Public project overview

**API Keys Location**: See API_STRUCTURES.md curl examples for FEC_API_KEY and CONGRESS_API_KEY

---

**Last Updated**: 2025-10-18
**Last Major Fix**: Itemized analysis 25 MB KV limit solved with chunked storage architecture
